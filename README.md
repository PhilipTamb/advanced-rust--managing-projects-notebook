# advanced-rust--managing-projects-notebook


Course Title: [Advanced Rust: Managing Projects](https://www.linkedin.com/learning/advanced-rust-managing-projects?)

Description: Are you looking for the next step to advance from the initial Rust essential training to manage growing Rust projects and writing automated test routines? In this course, instructor Barron Stone has exactly what you need. He explains how to manage projects using the Rust module system and explores topics such as using the “mod” keyword to define modules, navigating paths in Rust, and identifying parts of a program as public or private. Barron shows you how to automate tests, using test functions, assert! macros, custom failure messages, unit tests, integration tests, and more. This course can help you take the next step in your learning journey for mastering Rust.


***********************************************
Chapter: 1. Managing Projects
***********************************************


-----------------------------------------------
Video: The module system
-----------------------------------------------
Note Time:         Note Text:                     

0:01:42            Our third element of the module system are crates, which come in one of two variations. A binary crate is an executable program meant to run on its own and a library crate represents a collection of related code or library, which is meant to be used as part of another program. Now, the term crate is used somewhat loosely and you may hear it used to refer to either the source code used to build an executable program or library, the compiled artifacts produced by the build process, or even a compressed package fetched from a registry, such as crates.io, which is the default registry in the Rust ecosystem for third party crates. 

0:01:42            In addition to things like functions and macros, modules can also contain types, such as structs, enums, and unions. It can also contain traits and implementation blocks, constant and static values, and even other modules. To reference all of these items, we need the next feature of Rust's module system: paths, which are the way of naming and referring to items within the module system, so we can bring them into scope to use them in our program. 

0:01:42            Rust accomplishes those tasks through what's called the module system. Several key elements of the module system include, well, modules. We'll use modules to subdivide our code and group-related items to help organize it for readability and reuse. Modules provide isolated name spaces, allowing us to control the scope and privacy of items within a module. 

0:02:03            Finally, our last major feature of the Rust module system are packages, which are used to build, test, and share one or more crates that provide a set of functionality. A single package contains one or more targets, each of which is a crate. The package can have up to one library crate and zero to as many binary crates as you want as long as there's at least one crate. The package is organized with a directory structure, housing a collection of source files and a cargo.toml manifest file, which describes how to build the target crates, including information like external dependencies, the package name and version, and other necessary metadata. 


-----------------------------------------------
Video: Packages vs. crates
-----------------------------------------------
Note Time:         Note Text:                     

0:00:19            I'll type the command cargo build, and then to build all of the crates within the my project package, including the additional binary crates, I'll add the --workspace flag. After cargo is done compiling, I can navigate to the newly created target folder. And since cargo compiles in debug mode, by default, I'll go to the debug folder. Within there, we can see our compiled output files along with other compilation artifacts. My project.exe is the root binary crate for my project. Since I'm using Windows, it's an EXE file, but if you're on other operating systems, the executable format will be different. The compiler also produced EXE files for the second bin and third bin binary crates that are part of this package. And then last but not least, we also have the my project library file. By default, cargo adds a lib prefix to the front of library names. And the output file is given a .rlib file extension. Those are the three output artifacts produced from building the my project package, three binary executables, and a library that can be easily reused in other applications. In addition to the SRC directory which holds the source code, there are a few other optional directories that can be included in cargo packages. A benches directory to hold benchmarking code, an example directory to hold relevant example programs for the package, and a test directory to hold integration tests. 

0:00:19            the third bin crate. There's a lot going on here, so let's recap all the crates within this package named my project. Lib.rs within the source directory is the root of the my project library crate. And main.rs is the root of the my project binary crate. One or both of those crates might reference code contained within some module. Second bin.rs is the root of a second binary crate within this package, and main.rs within the third bin sub directory is the third binary crate. To demonstrate all that, I've recreated the package structure with three binary crates and one library crate here. Within main.rs for the top level root binary crate, I have a simple main function starting at line eight which prints a message that it's running the my project executable. Then it calls a function from some module on line 10 followed by a function from the my project library crate on line 11. Switching over to lib.rs, we can see that it contains a simple function named lib funk, which prints a message about where it's being called from. And over in some module, there's a mod funk function, which prints a similar message. Now digging down into the bin directory, looking at second bin.rs, we see that as the root for our second binary crate, it has its own main function. And then digging down even further into the third bin directory within its main.rs, we have yet another main function because it's our third binary crate, and it calls a function from another module. 

0:00:19            that directory which are code modules that can be used in our crates. Now, packages can only have, at most one library crate. However, a package can contain as many binary crates as you want. So, where do those go? Any additional binary crates go in a sub directory of the SRC directory named bin. Unlike main.rs, in the top level source code directory, which compiles to an executable with the package name, binary crates within the bin sub directory will compile to an executable named after that source file. So, second bin.rs will produce a binary executable named second bin. And third bin.rs will compile to produce an executable named third bin. Each of the source files within the bin directory represent additional binary crates within this my project package. And therefore the compiler will expect each of those files to contain its own main function as an entry point for that program. Now, what if we want to include additional source files containing code modules to be referenced by our third bin executable? If we just add another file within the bin directory, cargo expects that to be the root for another binary crate, and therefore should contain a main function. If our module is just a collection of related functions, types, traits, and so on, that's not going to work because that file is not meant to be compiled into its own executable. The way to organize binary crates with additional modules is to create a sub directory within the bin directory named for that crate. So for example, within this third bin directory, I have a file named main.rs which is the root of the third bin binary crate, and contains its main function. And I have another module to be used by the third bin crate. 

0:00:19            In short, a package contains one or more binary and/or library crates that provide a set of functionality. Now, to understand what that really means, let's explore the file structure of a package to see how crates fit into it. We can create a new cargo package using the console command cargo new followed by the name for our package, in this case, my_project. Cargo will then generate a default package, consisting of the files and directories shown here. 

At the top level, we have our root directory, which is given the package name by default. And within that route directory lives cargo.toml, which is the cargo manifest file describing how to build the crates within this package. There's not much to see in a newly generated cargo manifest, just a few basic fields of information about the package including its name, version, and the addition of Rust to use when compiling it. This manifest serves as the build instructions for our package, but notice it doesn't contain a reference to any source files. So, how does cargo know what to compile and build? When cargo goes to build this package, it knows to look in the source code directory, SRC, and in there it will see main.rs. This is the standard file name to indicate the root source file for a binary crate. And it's what cargo will pass to the Rust compiler. The compiler will expect main.rs to contain a function named main, which serves as the starting or entry point for the program. And it will compile that to produce a binary executable with the same name as the package. If we want our package to produce a library instead of an executable binary, we can use the --lib flag with the cargo new command, which will give us a package with a file named lib.rs in the source code directory. In the same way that cargo recognized a file named main.rs as the route of a binary crate, when it sees lib.rs in that source directory, it knows that is the root file to compile a library crate. And since a library is a set of functionality that's meant to be used by other programs and not run on its own, the compiler does not expect it to contain a main function like a binary crate needs to. If our source code directory contains both lib.rs and main.rs files, now our package has two crates, one library crate and one binary crate. This might be a project that has a lot of code and functionality we want to reuse later for other applications. We put all of that reusable code into the library crate which gets built, and then the binary crate uses it. Lib and main are the two special file names cargo looks for in the source directory, but we can also have source files with other names in that directory which are code modules that can be used in our crates. 


-----------------------------------------------
Video: Defining modules
-----------------------------------------------
Note Time:         Note Text:                     

0:00:32            A module can be defined in line within the source file that will be using that module, or the module's contents can be placed into a separate file.

 let's define an inline module to house several functions for saying hello in different languages. To define a new module, we use the keyword mod, followed by a name for the module, I'll name this one, hello, and then a pair of open and closed curly braces. Anything we put inside those brackets will be contained within this scope of this module.

Since we defined this inline module within main.rs it will automatically be brought into scope. So we'll be able to call its functions and use any other code we might add later to it from within this main source file. 

We could simply add more functions here, but it might make sense to organize our casual code into its own submodule within the hello module. To do that, I'll use the mod keyword again, within the scope of the hello module. 

However, thanks to the module system our two English functions are in different scopes each with its own unique path to reference it.

 


-----------------------------------------------
Video: Absolute vs. relative path
-----------------------------------------------
Note Time:         Note Text:                     

0:00:20            The modules shown here represent a hierarchy or module tree which starts from the root crate. For binary crates, the main.rs source file is the crate root, and for library crates, the lib.rs file serves as the crate root. So within our Hello, World package, we have main.rs, which is the crate root, and within that file, we define the inline module hello. That hello module contains two functions named english and spanish, as well as a submodule named casual. And within that casual module, we have another function named english. We can reference items within this hierarchical tree structure using paths. 

0:00:25            The modules shown here represent a hierarchy or module tree which starts from the root crate. For binary crates, the main.rs source file is the crate root, and for library crates, the lib.rs file serves as the crate root. So within our Hello, World package, we have main.rs, which is the crate root, and within that file, we define the inline module hello. That hello module contains two functions named english and spanish, as well as a submodule named casual. And within that casual module, we have another function named english. We can reference items within this hierarchical tree structure using paths. 

0:01:04            For example, our second english function is contained within the casual module, which itself is part of the hello module, which is within our root crate. So crate::hello::casual::english is a path to that specific english function. Notice that we use double colons to separate layers of this hierarchy. The type of path shown here is what's referred to as an absolute path because it starts from the root crate. You can recognize absolute paths because they start with the crate name, or simply the literal word crate. So, for example, if we want to call the english function to say hello, we can reference it as crate::hello::english. This will call the function on line six to print the message hello. Along this same lines, if we want to call the spanish function, we can use the absolute path crate::hello::spanish, which prints the message hola. And, finally, to call the casual english function, which is a submodule of hello, we can access it as crate::hello::casual::english. Absolute paths have the benefit of being explicit so it's easy to know exactly where each of these functions live within the module tree. However, as the module tree grows in size and complexity, absolute path names can grow quite long and cumbersome. That's why, in addition to absolute paths, rust also lets us reference items using a relative path, which starts from within the current scope. You can recognize a relative path because instead of beginning with the crate root, it starts with an identifier that's we within the scope of the current module, or keywords such as self or super. 

0:04:38            But what if we want to call functions from a higher level within that hierarchy? Let's say, from within our casual english function at line 20, we want to call the spanish function that's contained in the hello module. One way we can do that is using an absolute path, crate::hello::spanish. And since the absolute path starts with the crate route, there's no doubt as to where to find this spanish function. Now that clear and explicit nature of absolute paths can make them tempting to use for everything but there is a downside. 

0:05:27            If we want to build a relative path that starts from a higher level module, we can use the keyword super. Super tells rust to build the relative path starting with the parent of the current module. Since the casual module is contained within the now named greeting module, the relative path starts at greeting, that we can reference the spanish function directly from there. Learning to work with relative paths is important as your rust programs grow in size and complexity. 


-----------------------------------------------
Video: Public modules
-----------------------------------------------
Note Time:         Note Text:                     

0:00:08            By default, Rust makes all items defined within a module private. A parent module cannot access or use private items from within one of its children or sub-modules, however, a child module can access and use items from its parent or other ancestor modules even if those items are private. To explore that concept, let's use our hello module, with two functions to say hello in English and Spanish, and a sub-module named Casual to say, "Hey." Within the main function of this binary crate at line two, we call the hello modules English function. Now, if I try to compile and run this program with the command cargo run, we get an error which says that the function English is private. In order to call that English function from the top level root crate will need to explicitly market as public by adding the keyword pub to its definition I'll compile and run the program again. And now that English is public, it can be called and we get an output message that says, "Hello." 


-----------------------------------------------
Video: Public structs and enums
-----------------------------------------------
Note Time:         Note Text:                     

0:04:22            is controlled on a field by field basis separate from the overall visibility of the struct. Enums on the other hand deal with privacy slightly differently. An enum would not be very useful if its variance were not visible. So when an enum is declared as public all of its variants are also made public. For example, declaring this shape enum as public means the rectangle, circle and triangle variance of the enum will also be public without needing to explicitly annotate each one. 

0:04:22            So when dealing with the struct remember that the privacy of individual fields is controlled on a field by field basis separate from the overall visibility of the struct. 


-----------------------------------------------
Video: Bringing paths into scope
-----------------------------------------------
Note Time:         Note Text:                     

0:01:14            So to simplify the pads in our program, Rust allows us to write use statements which bring a specific path into scope so we can access items as if they were local.
i.e. 
use crate::greteing::formal; 

0:04:53            There's one more trick we can use to clean up large lists of use statements when using multiple items to find in the same crate or module such as the formal and casual modules used here, which are both part of the greetings module. Rather than putting those two statements on separate lines, we can use what's called a nested path to bring them both into scope with a single line. 

0:04:53            successfully calls the casual Spanish function. Although it's not mandatory, when importing functions, it's a common convention to import the function's parent module similar to the example we just saw, which imported the formal submodule and then included that in the path to call the Spanish function. However, when writing use statements for other types of items, such as structs or enums, it's common practice to bring that item itself into scope. For example, if I needed to use the HashMap struct from the collections module in my program, I would bring the struct itself into scope to access it directly. These aren't hard and fast rules, but they are standard conventions for writing idiomatic Rust code. 


-----------------------------------------------
Video: Using external crates
-----------------------------------------------
Note Time:         Note Text:                     

0:04:17            We can use the keyword "as" when writing use statements to specify a new name to use for the item. This is useful in situations where we need to import multiple different items that have the same name. For example, let's say we're using the standard IO and format modules, which both have their own type named Result. If we wrote some function that returns a result, it would be ambiguous which result type it's returning, the one from IO, or the one from format. This code would actually fail to compile, due to the name "Result" being imported twice. We can fix that by using the "as" keyword to define new unique names for each of those result types we're using, and then using those new names in our code, so now it's clear that this function is returning the IO result, and not the format result. Another handy tool when writing use statements is the glob, or global operator, it's represented with the asterisk symbol and is used to bring a all public items defined in this specified path into scope. So for example, the use statement shown here would bring everything defined in the standard IO path into scope. 


-----------------------------------------------
Video: Separating modules into multiple files
-----------------------------------------------
Note Time:         Note Text:                     

0:03:25            All of the modules we've defined so far in this course have been in-line modules, which are defined within the same file that uses them. Starting with the keyword mod, followed by a name for the module, the contents of the module is written right there, in-line. That works well for organizing relatively short modules with just a handful of items, but as the modules in your project grow in size and quantity, trying to cram everything into a single file does not scale very well. You'll probably want to move those modules into separate files to make the code easier to navigate and maintain. I'll refer to that as a "normal" module, where the contents of the module is defined in a separate .rs source file. A file using that module declares it using the mod keyword, which tells the Rust compiler where to look to find the corresponding code. To better explain that, let's look at the file structure for a Rust project. This is the simplified contents of a default cargo project with a single main source file. If I add another .rs file to the source code directory with a unique name, in this case, some_module, the code contained within that file will be part of the "some_module" module. However, simply adding that file to the source directory does not mean the a code within, main.rs, will know to find it there. We will need to define some_module within the source code using it with the mod keyword. When the Rust compiler sees this line, mod some_module in main.rs, it knows to look for another file named some_module containing the module contents within that same directory. Now let's say we want to create a submodule within the some_module module, which we'll refer to as its child, we can do that by creating a new directory named some_module, and then putting the source file for that submodule in there. In this example, it's named child_module.rs. And similar to how we needed to define some_module within main.rs to use it, we'll need to use the mod keyword to define child_module within some_module to use it. This example shows a common way to organize modules and submodules within Rust projects. However, there is a second alternative way these modules can be organized. Instead of putting the code for some_module in a source file named some_module, we can create a sub directory named some_module, and put the code in a source file named mod.rs. mod is a special file name that the Rust compiler knows to look for in a directory named after the module. If we want to give some_module a child submodule, we can do that by placing the child_module's source file in the some_module directory. These are two different ways to organize modules in Rust, and you can even use both methods within the same project. Personally, I think the first style on the left makes more sense, and it's how I recommend organizing your Rust projects. However, you should be familiar with the other style in case you encounter it in code from other developers. Let's look at an example that implements both styles of module 


***********************************************
Chapter: 2. Automating Tests
***********************************************


-----------------------------------------------
Video: Test functions
-----------------------------------------------
Note Time:         Note Text:                     

0:02:49            The general structure for a test function is fairly straightforward, it starts with initializing data in variables, and setting the program state to be ready for the test, then we actually run the code to execute the test, and lastly, check to make sure the results of the test are what we expected. In Rust, we can define test functions by annotating them with the test attribute, adding hashtag test in square brackets on the line before the function. Then if we call the cargo test command, it will build an executable to run all the functions annotated as test functions, and show us the results. 

0:03:13            To demonstrate that, I'll use cargo to create a new library with the command cargo new, the lib flag, and I'll name it "test_functions" 
"cargo new --lib test_function"
 . I'll use the cd command to navigate into the newly created directory, and I'll open the lib source file in the code editor. By default, cargo populates the new library with a basic template for unit tests. Notice that line three uses a hashtag attribute to annotate the it_works function line four as being a test.To run this test, I'll use the command cargo test. And after it runs, we can see the results. It ran one test, which was the it_works function, and the result was ok. Below that, we get a summary of the test results. One passed, zero failed, and so on. Rust also gives us the capability to test example code that's part of inline documentation, referred to as doc tests. We can add as many test functions as we want to this module. 


-----------------------------------------------
Video: assert! macro
-----------------------------------------------
Note Time:         Note Text:                     

0:00:42            One of the core mechanisms we'll use when writing test functions is the assert macro, which is included as part of the Rust standard library. The assert macro accepts a single argument that evaluates to a Boolean value of either true or false. If the value is true, that means the test passes and assert does nothing. However, if the argument evaluates to false, assert calls the panic! macro and the test fails. 


-----------------------------------------------
Video: Custom failure messages
-----------------------------------------------
Note Time:         Note Text:                     

0:00:58            The assert macro can only take in expressions that evaluate to a Boolean value of true or false. But what if we want to test a section of code that produces a non-Boolean result? convenient. The assert.equal macro accepts to input arguments and compares them for equality. If they are equal, the test passes and the assert macro does nothing. But if they're not equal, it fails and cause the panic macro. The counterpart to the assert macro is assert.notEqual which does the opposite comparison. If the two input arguments are not equal, that means the test passes. And if they're the same, then that's considered a failure. 

0:02:58            save that change and run those tests again. And now looking at the summary, we can see that the tests with assert and assert.equal both failed. However, the test with assert.notEqual passed. In this case, testing to make sure the result of adding two and two was not three, did not help us identify a problem with a different incorrect result. Now, let's look at the difference in output from our two tests that did fail. Notice that the output for the test with regular assert only shows us the expression that was passed to the assert macro, but it doesn't give us much more information to help figure out what happened. However, the test with assert.equals below that does give us some additional insights. It tells us that the assertion failed because the two arguments, referred to as left and right, were not equal and it shows us their values. The left argument was seven and right was four. This additional information can be helpful starting to track down the source of a problem. Under the hood, the assert.equal and assert.notEqual macros use the equality and inequality operators to compare the left and right arguments, which means both of those arguments need to implement the PartialOrd trait so that they can be compared. Additionally, since the output message for a failed assertion displays the argument values, they need to implement the Debug trait as well. The integers we used in this video already implement those traits by default, But if you've defined your own custom types, like struct or enum, you'll need to either implement those traits yourself or add the derive attribute to your struct or enum definition to derive a default implementation. 


-----------------------------------------------
Video: assert_eq! and assert_ne! macros
-----------------------------------------------
Note Time:         Note Text:                     

0:00:22            assert_eq! and assert_ne! macros
All three variations of the assert macro enable us to optionally provide a custom message to convey extra information that might help with debugging when an assertion fails. Any additional arguments passed after the argument required for the assert macro will be passed to the format macro to print along with the failure message. 


-----------------------------------------------
Video: should_panic! macro
-----------------------------------------------
Note Time:         Note Text:                     

0:01:22            should_panic! macro
It's natural to think that when your Rust code panics, that's a bad thing. Oh, no! Something went wrong. But errors are a normal part of programming life. Things will go wrong. And when they do, we'll often want our code to panic so it can detect and handle the problem appropriately. Since we may design parts of our code to panic in certain error situations, we need a way to test and verify that behavior, and we can do that by adding the should_panic attribute to a test function. With this attribute, if the code inside the test function panics, the test will pass, otherwise if the test function does not panic, then the test is considered a failure. 

0:02:34            To do that, I'll add the should_panic attribute after the test attribute, but before the function itself. This indicates that this specific test is supposed to panic. 

0:03:56            Now the basic should_panic attribute as I'm using it here will pass if the function panics for any reason. That means, if there were multiple reasons the function could panic, we might want to know why it did panic. We can restrict the should_panic attribute to only pass for specific types of panic by adding the expected parameter and passing it the panic message string we expect. If the function panics with a message containing the string A Person needs a name!, the test will pass. If it panics for any other reason, this test will fail. In fact, we don't even need to include the entire panic message for the expected parameter, just a subset. 


-----------------------------------------------
Video: Controlling test execution
-----------------------------------------------
Note Time:         Note Text:                     

0:01:22            Controlling test execution
 When we use the "cargo test" command, it compiles our code in test mode, and runs the resulting test binary executable. By default, if there are multiple tests, it will run them in parallel, and it will capture any standard output generated during those test runs, so you will not see the output of things like print statements on the console. That cleans up the output, making the test results easier to read, but it may hide additional information that you want to see. As with many command line utilities, we can pass arguments to control how cargo tests are executed. The "cargo test" command takes two types of arguments, separated by a pair of dashes or hyphens. Arguments placed before the hyphen separator are passed to the cargo test utility and arguments that come after the separator go to the test binary itself. So, running the "cargo test" help command will display a list of options you can use with "cargo test" on the left side of the hyphen separator, and running that command with the help argument after the separator, will display a list of options you can use on the right side for the test binary. 

0:03:16            If we want to see everything that's printed to standard output by all of the tests, we can do that by calling "cargo test." And then after the separator, use the "show-output" flag. When I run that command, now we can see all of the standard output. The messages from the two test functions that passed are organized under the successes section, and the output from the positive numbers test, which fails, is under the failures section. Now, as I mentioned earlier in this video, when you run multiple tests, by default, they'll use multiple threads to take advantage of multi-core processors and run the tests in parallel. That means that tests will finish running faster so you get feedback sooner about which tests passed or failed. However, running the test functions at the same time can potentially cause problems if there's some sort of dependency or shared information between the tests. For example, if multiple test functions write data to the same file and then read it back, depending on the timing and order in which those operations occur, a function could end up reading data that was written by another function, which is not what they were expecting to see. And thus, the test would fail. An easy fix might be to give each function its own unique file to read and write. However, depending on the situation, this type of simple separation may not always be possible. When dependencies do exist between the test functions, you may need to run them sequentially one at a time. It'll take longer for all the tests to finish running, but they avoid potentially interfering with each other. We can control the number of threads used to run the tests by passing the "test threads" flag to the test binary. If I set the number of threads equal to one, that will prevent the tests from running in parallel, by limiting them to a single thread. Now, in the case of my simple test functions here, that doesn't really matter, because they're completely independent from one another, but we can still set that restriction. I'll run those tests. And we get a similar output as before, but now under the hood, those three tests are running sequentially. Since this example only has three test functions, and they're all short and simple, they finish running quickly even with just a single thread. However, if I had a full test suite with hundreds or thousands of test functions, it could take a long time for everything to run. So, there might be times when I only want to run a limited subset of all of the tests. We can tell Cargo to run a single specific test, by passing its name to the "cargo test" command. For example, if I call "cargo test," and pass it the name "test zero," that will only execute the "test zero" function. On the last line of output, summarizing the test results, we can see that two tests were filtered out. Now, Cargo only accepts a single argument for the test name. However, the name we give it does not have to be a full proper name pointing to one specific test. Cargo will take whatever string we give it, and then run all tests containing that string within their name, which is how we can filter to run multiple tests. For example, let's say I only want to run the two test functions for positive and negative numbers. They both have the word "numbers" in their name. So, I'll call "cargo test," and pass it the test name numbers. When I run that, we see that the positive and negative numbered tests were run and the "test zero" function was filtered out. One way we can control which tests to run is with the "ignore" attribute. Let's say the negative numbers test was incredibly time-consuming for some reason. It takes hours to run, so I typically don't want to execute it as a normal part of my test suite. I can tell Cargo to exclude it by adding the "ignore" attribute after the "test" attribute. I'll save and run that with the basic "cargo test" command, and the output shows that the "test negative numbers" function was selectively ignored. If later I have a bunch of time to run the expensive tests that I previously ignored, I can do that by calling "cargo test," and passing the "ignored" flag to the test binary. Now, only the "negative numbers" test which was marked as "ignored" is executed, and the other two tests are filtered out. Keep these various filtering mechanisms in mind as you're building and naming your test functions so you can easily control which tests to run. 


-----------------------------------------------
Video: Unit tests
-----------------------------------------------
Note Time:         Note Text:                     

0:00:55            Unit tests
 In Rust, tests are categorized into two main types: unit tests and integration tests. We'll focus on unit tests for this video and then look at integration tests in the next one. Unit tests are written to test individual units of code like modules in isolation and they tend to be relatively short. The unit test code is contained within the same file as the code it's testing and the standard convention is to organize all the unit tests into a module named tests, which is is annotated with the CFG test attribute. These are the type of tests we've seen thus far in this course. Including that CFG attribute indicates conditional compilation. So Rust will only compile code in the test module when you run the cargo test command. That can save us precious compilation time and keep the resulting compilation artifacts smaller when building our projects for reasons other than test. 

0:03:22            Rust does not define an official naming convention for unit test functions, but I've chosen to give each of these three functions names with the prefix ut to identify them as unit tests. Including name elements like this can come in handy when filtering which tests to run as shown in the previous video. That said, in a large application with a ton of test functions, a short two-letter combination like ut is likely to show up elsewhere in other test function names and therefore might not be a useful filtering mechanism, so come up with something that makes sense for your project. 


-----------------------------------------------
Video: Integration tests
-----------------------------------------------
Note Time:         Note Text:                     

0:01:08            Integration tests
 If our Rust project contains a bunch of different modules and we include unit tests for each one, just because all of those unit tests pass individually in isolation, that does not guarantee that when we use all those modules together at a higher level in the application, there won't be any problems. And this is where tests come into play. Unlike unit tests, which live in the same file as the code they're testing, in Rust integration tests live outside of the libraries they test. Their job is to test your code as if they were any other external code using it. Only accessing it through the public interface. Integration tests often exercise multiple modules or libraries at the same time to make sure they'll work well together. Rust looks for integration tests in a top level directory named tests which sits next to the source directory. We can add as many test files to that directory as we want. And when we run the Cargo test command it compile each of those files into an individual test binary crate to be run. 

0:03:12            If you only want to run the integration tests and skip all the unit testing, you can do so by calling cargo test and passing the test flag argument followed by an asterisk enclosed by single quotes. That sets a filter to only run our two integration tests. Along those lines, if we only wanted to run the unit tests contained within the library, we can do so by calling cargo test and passing it the live flag. That filters out and ignores the integration tests so that only these seven unit tests are run. Now, there are a couple of things to keep in mind when developing integration tests. Let's say we want to organize a bunch of common support code to help with testing into a separate utilities module, which is then declared and used within the other integration test files. Since Rust treats every source file at the top level of the test directory as an integration test, this test utilities file will also get compiled into a test binary and run and show up in the test output. Even if it doesn't actually contain any annotated test functions. The workaround to prevent that from happening is to use the alternate structure for modules moving the code into a file named mod.rs within a sub directory with the module name, test_utils. Another important consideration when writing integration tests is the fact that only library crates expose their functionality for other crates to import and use. Binary crates are intended to run on their own and cannot be used by other crates. Therefore, we cannot write integration tests to directly test a binary crate. So if your project only contains a binary crate, you will not be able to write an integration tests for it. The solution and common way to structure Rust programs is to move as much of the functionality from the binary crate into a library crate which can be used for integration testing. Once you've tested to make sure everything important within that library is working correctly, the small amount of code remaining in the main source file that uses it should also work and not require extensive testing. 

0:03:16            If you only want to run the integration tests and skip all the unit testing, you can do so by calling cargo test and passing the test flag argument followed by an asterisk enclosed by single quotes. That sets a filter to only run our two integration tests. Along those lines, if we only wanted to run the unit tests contained within the library, we can do so by calling cargo test and passing it the live flag. That filters out and ignores the integration tests so that only these seven unit tests are run. Now, there are a couple of things to keep in mind when developing integration tests. Let's say we want to organize a bunch of common support code to help with testing into a separate utilities module, which is then declared and used within the other integration test files. Since Rust treats every source file at the top level of the test directory as an integration test, this test utilities file will also get compiled into a test binary and run and show up in the test output. Even if it doesn't actually contain any annotated test functions. The workaround to prevent that from happening is to use the alternate structure for modules moving the code into a file named mod.rs within a sub directory with the module name, test_utils. Another important consideration when writing integration tests is the fact that only library crates expose their functionality for other crates to import and use. Binary crates are intended to run on their own and cannot be used by other crates. Therefore, we cannot write integration tests to directly test a binary crate. So if your project only contains a binary crate, you will not be able to write an integration tests for it. The solution and common way to structure Rust programs is to move as much of the functionality from the binary crate into a library crate which can be used for integration testing. Once you've tested to make sure everything important within that library is working correctly, the small amount of code remaining in the main source file that uses it should also work and not require extensive testing. 

0:03:18            If you only want to run the integration tests and skip all the unit testing, you can do so by calling cargo test and passing the test flag argument followed by an asterisk enclosed by single quotes. That sets a filter to only run our two integration tests. Along those lines, if we only wanted to run the unit tests contained within the library, we can do so by calling cargo test and passing it the live flag. That filters out and ignores the integration tests so that only these seven unit tests are run. Now, there are a couple of things to keep in mind when developing integration tests. Let's say we want to organize a bunch of common support code to help with testing into a separate utilities module, which is then declared and used within the other integration test files. Since Rust treats every source file at the top level of the test directory as an integration test, this test utilities file will also get compiled into a test binary and run and show up in the test output. Even if it doesn't actually contain any annotated test functions. The workaround to prevent that from happening is to use the alternate structure for modules moving the code into a file named mod.rs within a sub directory with the module name, test_utils. Another important consideration when writing integration tests is the fact that only library crates expose their functionality for other crates to import and use. Binary crates are intended to run on their own and cannot be used by other crates. Therefore, we cannot write integration tests to directly test a binary crate. So if your project only contains a binary crate, you will not be able to write an integration tests for it. The solution and common way to structure Rust programs is to move as much of the functionality from the binary crate into a library crate which can be used for integration testing. Once you've tested to make sure everything important within that library is working correctly, the small amount of code remaining in the main source file that uses it should also work and not require extensive testing. 

0:03:18            If you only want to run the integration tests and skip all the unit testing, you can do so by calling cargo test and passing the test flag argument followed by an asterisk enclosed by single quotes. That sets a filter to only run our two integration tests. Along those lines, if we only wanted to run the unit tests contained within the library, we can do so by calling cargo test and passing it the live flag. That filters out and ignores the integration tests so that only these seven unit tests are run. Now, there are a couple of things to keep in mind when developing integration tests. Let's say we want to organize a bunch of common support code to help with testing into a separate utilities module, which is then declared and used within the other integration test files. Since Rust treats every source file at the top level of the test directory as an integration test, this test utilities file will also get compiled into a test binary and run and show up in the test output. Even if it doesn't actually contain any annotated test functions. The workaround to prevent that from happening is to use the alternate structure for modules moving the code into a file named mod.rs within a sub directory with the module name, test_utils. Another important consideration when writing integration tests is the fact that only library crates expose their functionality for other crates to import and use. Binary crates are intended to run on their own and cannot be used by other crates. Therefore, we cannot write integration tests to directly test a binary crate. So if your project only contains a binary crate, you will not be able to write an integration tests for it. The solution and common way to structure Rust programs is to move as much of the functionality from the binary crate into a library crate which can be used for integration testing. Once you've tested to make sure everything important within that library is working correctly, the small amount of code remaining in the main source file that uses it should also work and not require extensive testing. 

0:03:19            If you only want to run the integration tests and skip all the unit testing, you can do so by calling cargo test and passing the test flag argument followed by an asterisk enclosed by single quotes. That sets a filter to only run our two integration tests. Along those lines, if we only wanted to run the unit tests contained within the library, we can do so by calling cargo test and passing it the live flag. That filters out and ignores the integration tests so that only these seven unit tests are run. Now, there are a couple of things to keep in mind when developing integration tests. Let's say we want to organize a bunch of common support code to help with testing into a separate utilities module, which is then declared and used within the other integration test files. Since Rust treats every source file at the top level of the test directory as an integration test, this test utilities file will also get compiled into a test binary and run and show up in the test output. Even if it doesn't actually contain any annotated test functions. The workaround to prevent that from happening is to use the alternate structure for modules moving the code into a file named mod.rs within a sub directory with the module name, test_utils. Another important consideration when writing integration tests is the fact that only library crates expose their functionality for other crates to import and use. Binary crates are intended to run on their own and cannot be used by other crates. Therefore, we cannot write integration tests to directly test a binary crate. So if your project only contains a binary crate, you will not be able to write an integration tests for it. The solution and common way to structure Rust programs is to move as much of the functionality from the binary crate into a library crate which can be used for integration testing. Once you've tested to make sure everything important within that library is working correctly, the small amount of code remaining in the main source file that uses it should also work and not require extensive testing. 

0:03:20            If you only want to run the integration tests and skip all the unit testing, you can do so by calling cargo test and passing the test flag argument followed by an asterisk enclosed by single quotes. That sets a filter to only run our two integration tests. Along those lines, if we only wanted to run the unit tests contained within the library, we can do so by calling cargo test and passing it the live flag. That filters out and ignores the integration tests so that only these seven unit tests are run. Now, there are a couple of things to keep in mind when developing integration tests. Let's say we want to organize a bunch of common support code to help with testing into a separate utilities module, which is then declared and used within the other integration test files. Since Rust treats every source file at the top level of the test directory as an integration test, this test utilities file will also get compiled into a test binary and run and show up in the test output. Even if it doesn't actually contain any annotated test functions. The workaround to prevent that from happening is to use the alternate structure for modules moving the code into a file named mod.rs within a sub directory with the module name, test_utils. Another important consideration when writing integration tests is the fact that only library crates expose their functionality for other crates to import and use. Binary crates are intended to run on their own and cannot be used by other crates. Therefore, we cannot write integration tests to directly test a binary crate. So if your project only contains a binary crate, you will not be able to write an integration tests for it. The solution and common way to structure Rust programs is to move as much of the functionality from the binary crate into a library crate which can be used for integration testing. Once you've tested to make sure everything important within that library is working correctly, the small amount of code remaining in the main source file that uses it should also work and not require extensive testing. 

0:03:20            If you only want to run the integration tests and skip all the unit testing, you can do so by calling cargo test and passing the test flag argument followed by an asterisk enclosed by single quotes. That sets a filter to only run our two integration tests. Along those lines, if we only wanted to run the unit tests contained within the library, we can do so by calling cargo test and passing it the live flag. That filters out and ignores the integration tests so that only these seven unit tests are run. Now, there are a couple of things to keep in mind when developing integration tests. Let's say we want to organize a bunch of common support code to help with testing into a separate utilities module, which is then declared and used within the other integration test files. Since Rust treats every source file at the top level of the test directory as an integration test, this test utilities file will also get compiled into a test binary and run and show up in the test output. Even if it doesn't actually contain any annotated test functions. The workaround to prevent that from happening is to use the alternate structure for modules moving the code into a file named mod.rs within a sub directory with the module name, test_utils. Another important consideration when writing integration tests is the fact that only library crates expose their functionality for other crates to import and use. Binary crates are intended to run on their own and cannot be used by other crates. Therefore, we cannot write integration tests to directly test a binary crate. So if your project only contains a binary crate, you will not be able to write an integration tests for it. The solution and common way to structure Rust programs is to move as much of the functionality from the binary crate into a library crate which can be used for integration testing. Once you've tested to make sure everything important within that library is working correctly, the small amount of code remaining in the main source file that uses it should also work and not require extensive testing. 

